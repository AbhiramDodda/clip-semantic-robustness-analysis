{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abhiramd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"lmms-lab/flickr30k\", split=\"test\")\n",
    "dataset_subset = dataset.select(range(1000))\n",
    "\n",
    "images = [item['image'] for item in dataset_subset]\n",
    "original_captions = [item['caption'][0] for item in dataset_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'Recall@1': 65.9000039100647, 'Recall@5': 87.30000257492065, 'Recall@10': 92.70000457763672}\n"
     ]
    }
   ],
   "source": [
    "def retrieval_metrics(model, processor, images, captions, device):\n",
    "    K=[1, 5, 10]\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)['pixel_values'].to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(image_inputs)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity_matrix = text_features @ image_features.T\n",
    "\n",
    "    results = {}\n",
    "    for k in K:\n",
    "        top_k_indices = torch.argsort(similarity_matrix, dim=1, descending=True)[:, :k]\n",
    "        correct_indices = torch.arange(len(captions)).unsqueeze(1).to(device)\n",
    "        is_correct = (top_k_indices == correct_indices).any(dim=1).float()\n",
    "        recall_at_k = is_correct.mean().item()\n",
    "        results[f\"Recall@{k}\"] = recall_at_k * 100\n",
    "\n",
    "    return results\n",
    "baseline_results = retrieval_metrics(model, processor, images, original_captions, device)\n",
    "print(f\"Baseline: {baseline_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return None\n",
    "\n",
    "def synonym_replacement_attack(caption, replacement_ratio=0.25):\n",
    "    tokens = word_tokenize(caption)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    replaceable_words = []\n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag and word.lower() not in nltk.corpus.stopwords.words('english'):\n",
    "             replaceable_words.append((i, word, wn_tag))\n",
    "\n",
    "    num_to_replace = int(len(replaceable_words) * replacement_ratio)\n",
    "\n",
    "    np.random.shuffle(replaceable_words)\n",
    "    words_to_replace = replaceable_words[:num_to_replace]\n",
    "    \n",
    "    new_tokens = list(tokens)\n",
    "    \n",
    "    for index, original_word, wn_tag in words_to_replace:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(original_word, pos=wn_tag):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name().lower() != original_word.lower():\n",
    "                    synonyms.append(lemma.name().replace('_', ' ')) \n",
    "        \n",
    "        if synonyms:\n",
    "            new_tokens[index] = np.random.choice(synonyms)\n",
    "            \n",
    "    return \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "paraphraser_model_name = \"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    "paraphraser_tokenizer = AutoTokenizer.from_pretrained(paraphraser_model_name)\n",
    "paraphraser_model = AutoModelForSeq2SeqLM.from_pretrained(paraphraser_model_name).to(device)\n",
    "\n",
    "def paraphrasing_attack(caption, num_paraphrases=3):\n",
    "    input_text = f\"paraphrase: {caption}\"\n",
    "    inputs = paraphraser_tokenizer(input_text, return_tensors=\"pt\", \n",
    "                                   padding=True, truncation=True).to(device)\n",
    "\n",
    "    outputs = paraphraser_model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=num_paraphrases,\n",
    "        do_sample=True, \n",
    "        temperature=1.0 \n",
    "    )\n",
    "    \n",
    "    paraphrases = [paraphraser_tokenizer.decode(output, skip_special_tokens=True) \n",
    "                   for output in outputs]\n",
    "    return paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement(word, wn_tag, specificity):\n",
    "    synsets = wordnet.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    synset = synsets[0]\n",
    "    \n",
    "    if specificity == 'hypernym':\n",
    "        generalizations = synset.hypernyms()\n",
    "    else:\n",
    "        generalizations = synset.hyponyms()\n",
    "        \n",
    "    if generalizations:\n",
    "        replacement = np.random.choice(generalizations).lemmas()[0].name().replace('_', ' ')\n",
    "        return replacement\n",
    "    \n",
    "    return None\n",
    "\n",
    "def specificity_attack(caption, specificity, replacement_ratio=0.25):\n",
    "    tokens = word_tokenize(caption)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    replaceable_nouns = []\n",
    "    for i, (word, tag) in enumerate(tagged_tokens):\n",
    "        if tag.startswith('N') and word.lower() not in nltk.corpus.stopwords.words('english'):\n",
    "             replaceable_nouns.append((i, word, wordnet.NOUN))\n",
    "\n",
    "    num_to_replace = int(len(replaceable_nouns) * replacement_ratio)\n",
    "    np.random.shuffle(replaceable_nouns)\n",
    "    words_to_replace = replaceable_nouns[:num_to_replace]\n",
    "    \n",
    "    new_tokens = list(tokens)\n",
    "    \n",
    "    for index, original_word, wn_tag in words_to_replace:\n",
    "        replacement = get_replacement(original_word, wn_tag, specificity)\n",
    "        if replacement:\n",
    "            new_tokens[index] = replacement\n",
    "            \n",
    "    return \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:06<00:48,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Synonym_25': {'Recall@1': 57.10000395774841, 'Recall@5': 82.10000395774841, 'Recall@10': 88.40000629425049}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:13<00:41,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Synonym_50': {'Recall@1': 45.00000178813934, 'Recall@5': 74.30000305175781, 'Recall@10': 82.50000476837158}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:21<00:36,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Synonym_75': {'Recall@1': 37.50000298023224, 'Recall@5': 61.500000953674316, 'Recall@10': 73.60000610351562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:28<00:28,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Hypernym_25': {'Recall@1': 60.700005292892456, 'Recall@5': 83.90000462532043, 'Recall@10': 89.80000615119934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:36<00:22,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Hypernym_50': {'Recall@1': 49.400001764297485, 'Recall@5': 76.60000324249268, 'Recall@10': 85.10000109672546}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:43<00:14,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Hyponym_25': {'Recall@1': 60.200005769729614, 'Recall@5': 83.60000252723694, 'Recall@10': 90.10000228881836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [00:50<00:07,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Hyponym_50': {'Recall@1': 51.90000534057617, 'Recall@5': 77.80000567436218, 'Recall@10': 84.80000495910645}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:57<00:00,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack 'Paraphrase': {'Recall@1': 59.90000367164612, 'Recall@5': 83.90000462532043, 'Recall@10': 90.6000018119812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = {\"baseline\": baseline_results}\n",
    "synonym_25_captions = [synonym_replacement_attack(c, 0.25) for c in original_captions]\n",
    "synonym_50_captions = [synonym_replacement_attack(c, 0.50) for c in original_captions]\n",
    "synonym_75_captions = [synonym_replacement_attack(c, 0.75) for c in original_captions] \n",
    "\n",
    "hypernym_25_captions = [specificity_attack(c, 'hypernym', 0.25) for c in original_captions]\n",
    "hypernym_50_captions = [specificity_attack(c, 'hypernym', 0.50) for c in original_captions] \n",
    "hyponym_25_captions = [specificity_attack(c, 'hyponym', 0.25) for c in original_captions]\n",
    "hyponym_50_captions = [specificity_attack(c, 'hyponym', 0.50) for c in original_captions] \n",
    "\n",
    "paraphrase_captions = [paraphrasing_attack(c, 1)[0] for c in original_captions]\n",
    "\n",
    "caption_sets_to_test = {\n",
    "    \"Synonym_25\": synonym_25_captions,\n",
    "    \"Synonym_50\": synonym_50_captions,\n",
    "    \"Synonym_75\": synonym_75_captions,\n",
    "    \"Hypernym_25\": hypernym_25_captions,\n",
    "    \"Hypernym_50\": hypernym_50_captions,\n",
    "    \"Hyponym_25\": hyponym_25_captions,\n",
    "    \"Hyponym_50\": hyponym_50_captions, \n",
    "    \"Paraphrase\": paraphrase_captions,\n",
    "}\n",
    "\n",
    "for attack_type, attacked_captions in tqdm(caption_sets_to_test.items()):\n",
    "    results = retrieval_metrics(model, processor, images, attacked_captions, device)\n",
    "    all_results[attack_type] = results\n",
    "    print(f\"Attack '{attack_type}': {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table:\n",
      "             Recall@1  Recall@5  Recall@10  R@1 Drop (%)\n",
      "baseline         65.9      87.3       92.7          0.00\n",
      "Synonym_25       57.1      82.1       88.4         13.35\n",
      "Synonym_50       45.0      74.3       82.5         31.71\n",
      "Synonym_75       37.5      61.5       73.6         43.10\n",
      "Hypernym_25      60.7      83.9       89.8          7.89\n",
      "Hypernym_50      49.4      76.6       85.1         25.04\n",
      "Hyponym_25       60.2      83.6       90.1          8.65\n",
      "Hyponym_50       51.9      77.8       84.8         21.24\n",
      "Paraphrase       59.9      83.9       90.6          9.10\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(all_results).T\n",
    "df = df.reindex(['baseline'] + [idx for idx in df.index if idx != 'baseline'])\n",
    "\n",
    "attack_types = df.index.to_list()\n",
    "R_k_values = ['Recall@1', 'Recall@5', 'Recall@10']\n",
    "colors = ['#E74C3C', '#F1C40F', '#008000']\n",
    "\n",
    "def autolabel(ax, rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3), \n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for rk_index, rk_metric in enumerate(R_k_values):\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    rects = ax.bar(attack_types, df[rk_metric], color=colors[rk_index])\n",
    "\n",
    "    ax.set_ylabel(f'{rk_metric} (%)', fontsize=12)\n",
    "    ax.set_title(f'CLIP Retrieval Performance: {rk_metric}', fontsize=14, pad=15)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticks(range(len(attack_types)))\n",
    "    ax.set_xticklabels(attack_types, rotation=45, ha=\"right\", fontsize=10)\n",
    "    \n",
    "    autolabel(ax, rects)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    file_name = f'{rk_metric.lower().replace(\"@\", \"_\")}_plot_final.png'\n",
    "    plt.savefig(file_name)\n",
    "    plt.close(fig) \n",
    "\n",
    "df_display = df.copy()\n",
    "baseline_r1 = df_display.loc['baseline', 'Recall@1']\n",
    "df_display['R@1 Drop (%)'] = ((baseline_r1 - df_display['Recall@1']) / baseline_r1 * 100).round(2)\n",
    "\n",
    "print(\"\\nTable:\")\n",
    "print(df_display.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retrieval_metrics_and_ranks(model, processor, images, captions, device):\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)['pixel_values'].to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(image_inputs)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity_matrix = text_features @ image_features.T \n",
    "    sorted_indices = torch.argsort(similarity_matrix, dim=1, descending=True)\n",
    "    correct_indices = torch.arange(len(captions)).to(device).unsqueeze(1)\n",
    "    ranks = (sorted_indices == correct_indices).nonzero(as_tuple=True)[1] + 1\n",
    "    results = {}\n",
    "    for k in [1, 5, 10]: \n",
    "        is_correct = (ranks <= k).float()\n",
    "        results[f\"Recall@{k}\"] = is_correct.mean().item() * 100 \n",
    "        \n",
    "    return similarity_matrix, ranks.cpu().numpy(), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "219 R@1 failure indices\n",
      "Index: 3\n",
      "Original Caption (Baseline Rank 1): Someone in a blue shirt and hat is standing on stair and leaning against a window .\n",
      "Attacked Caption (R@3): person in a blue shirt and hat is stand on step and leaning against a windowpane .\n",
      "FAILURE: 927 (Caption: A security officer with a tiny face and big glasses leans on a metal gate looking into the camera .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 4\n",
      "Original Caption (Baseline Rank 1): Two men  one in a gray shirt  one in a black shirt  standing near a stove .\n",
      "Attacked Caption (R@2): Two Isle of Man one in a gray shirt one in a black shirt standing near a kitchen range .\n",
      "FAILURE: 440 (Caption: A youth with long dirty blond-hair wearing a denim jacket and jeans looks at the conveyor belt of groceries while waiting at a checkout line .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 11\n",
      "Original Caption (Baseline Rank 1): Three young men and a young woman wearing sneakers are leaping in midair at the top of a flight of concrete stairs .\n",
      "Attacked Caption (R@2): Three young men and a new womanhood wearing stoolie are jump off in midair at the round top of a flying of concrete stairs .\n",
      "FAILURE: 912 (Caption: A child in a black cape is being chased by other children .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 14\n",
      "Original Caption (Baseline Rank 1): Some women are standing in front of a bus with buildings behind it .\n",
      "Attacked Caption (R@4): Some cleaning lady are stand in front of a bus with buildings behind it .\n",
      "FAILURE: 178 (Caption: A person is sleeping on a bench on the sidewalk  across the street from a bus stop .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 19\n",
      "Original Caption (Baseline Rank 1): Two women  both wearing glasses  are playing clarinets and an elderly woman is playing a stringed instrument .\n",
      "Attacked Caption (R@5): Two fair sex both wear thin methamphetamine hydrochloride are playing clarinets and an elderly cleaning woman is playing a stringed instrument .\n",
      "FAILURE: 95 (Caption: A little boy looks at the camera while a woman behind him seems to be laughing very hard and the woman on the right has a big smile on her face .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 20\n",
      "Original Caption (Baseline Rank 1): A person in gray stands alone on a structure outdoors in the dark .\n",
      "Attacked Caption (R@4): A person in gray stands unaccompanied on a construction open in the dark .\n",
      "FAILURE: 524 (Caption: Two men in hard hats on the roof of a building that is under construction .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 21\n",
      "Original Caption (Baseline Rank 1): A man in a white t-shirt looks toward the camera surrounded by a crowd near a metro station .\n",
      "Attacked Caption (R@3): A human beings in a snowy t-shirt looks toward the tv camera surrounded by a crowd near a metro station .\n",
      "FAILURE: 212 (Caption: A young man is walking through a busy street beside a mall .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 23\n",
      "Original Caption (Baseline Rank 1): two children  a girl and a boy are practicing their writing .\n",
      "Attacked Caption (R@6): two child a girl and a boy are use their writing .\n",
      "FAILURE: 773 (Caption: Three children are playing outdoors  two of them on a metal barrel swing .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 24\n",
      "Original Caption (Baseline Rank 1): A man in a blue hard hat and orange safety vest stands in an intersection while holding a flag .\n",
      "Attacked Caption (R@42): A man in a racy hard lid and Orange River rubber vest stands in an intersection while holding a sword lily .\n",
      "FAILURE: 417 (Caption: A shirtless man with sunglasses and a helmet is riding a motorcycle next to traffic cones .) as the top match.\n",
      "-------------------------------------------------------------\n",
      "Index: 28\n",
      "Original Caption (Baseline Rank 1): A small child grips onto the red ropes at the playground .\n",
      "Attacked Caption (R@116): A small minor travelling bag onto the red ropes at the resort area .\n",
      "FAILURE: 914 (Caption: A man is hiking on a mountain path with an American flag on his backpack .) as the top match.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "baseline_matrix, baseline_ranks, baseline_results = calculate_retrieval_metrics_and_ranks(model, processor, images, original_captions, device)\n",
    "synonym_captions = [synonym_replacement_attack(c, 0.50) for c in original_captions]\n",
    "synonym_matrix, synonym_ranks, synonym_results = calculate_retrieval_metrics_and_ranks(model, processor, images, synonym_captions, device)\n",
    "\n",
    "baseline_success = (baseline_ranks == 1)\n",
    "attack_failure = (synonym_ranks > 1)\n",
    "\n",
    "failure_indices = np.where(baseline_success & attack_failure)[0]\n",
    "print(f\"\\n{len(failure_indices)} R@1 failure indices\")\n",
    "\n",
    "picked = failure_indices[:10]\n",
    "\n",
    "for i in picked:\n",
    "    original_caption = original_captions[i]\n",
    "    attacked_caption = synonym_captions[i]\n",
    "    top_incorrect_image_index = synonym_matrix[i].argsort(descending=True)[0].item()\n",
    "    if top_incorrect_image_index == i:\n",
    "        top_retrieved_caption = \"(Correct Image Retrieved, skipping this index)\"\n",
    "        continue\n",
    "    top_retrieved_caption = original_captions[top_incorrect_image_index]\n",
    "\n",
    "    print(f\"Index: {i}\")\n",
    "    print(f\"Original Caption (Baseline Rank 1): {original_caption}\")\n",
    "    print(f\"Attacked Caption (R@{synonym_ranks[i]}): {attacked_caption}\")\n",
    "    print(f\"FAILURE: {top_incorrect_image_index} (Caption: {top_retrieved_caption}) as the top match.\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
